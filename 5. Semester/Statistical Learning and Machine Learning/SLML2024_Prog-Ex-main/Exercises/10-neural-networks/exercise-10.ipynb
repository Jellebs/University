{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Neural Networks\n",
    "\n",
    "Learning contents:\n",
    "1. Detailed example: Two-layer MLP for regression\n",
    "    - Forward pass: Calculate the values of $z_1$, $z_2$, and $y$\n",
    "    - Compute the mean squared error\n",
    "    - Using backpropagation, compute the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "    - Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "2. PyTorch: 2-layer MLP for classification\n",
    "    - Create and train a 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Detailed example: Two-layer MLP for regression\n",
    "We'll be working through a forward and back-propagation example in all its details for a 2-layer MLP for regression. Our network has the following structure:\n",
    "\n",
    "![](two-layer-nn.svg)\n",
    "\n",
    "Where \n",
    "    $$\n",
    "    z_j = \\text{ReLU}\\left(a_j \\right)\n",
    "    \\qquad\n",
    "    a_j = \\sum_i w^{(1)}_{ij} x_i\n",
    "    \\qquad\n",
    "    y_j = \\sum_i w^{(2)}_{ij} z_i\n",
    "    $$\n",
    "and the biases \n",
    "    $$\n",
    "    x_0 = z_0 = 1\n",
    "    $$\n",
    "\n",
    "Suppose, we have the weights\n",
    "    $$\n",
    "    \\mathbf{W}^{(1)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 & 0.2\\\\\n",
    "        -1.1 & 1.2\\\\\n",
    "        -2.1 & 2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    \\qquad\n",
    "    \\mathbf{w}^{(2)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        -0.1\\\\\n",
    "        1.1 \\\\\n",
    "        2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    $$\n",
    "Notice that the bias weights are included in the weight matrix. Relating it to the drawing, we have $w_{0,1} = 0.1$, $w_{1,2} = 1.2$\n",
    "\n",
    "Moreover, we are given an input\n",
    "    $$\n",
    "    \\mathbf{x} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 \\\\\n",
    "        0.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    $$\n",
    "    \n",
    "Relating these to the drawing, we have $w^{(1)}_{1,2} = 1.2$ and $x_1 = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1) Forward pass: Calculate the values of $z_1$, $z_2$, and $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the values and formulas given above, as well as information about the ReLU activation function given in lecture 19 sldie 9 to determine the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "activator = lambda a : np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Compute the mean squared error\n",
    "Suppose our target $t=2$.\n",
    "Use the formula for MSE shown in lecture 19, slide 17 (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Using backpropagation, compute the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "This corresponds to the bottom row of weights on the figure\n",
    "\n",
    "To compute this, look at formulas 5.65 - 5.67 in the course book(Bishop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "Use a learning rate $\\eta = 0.1$ \n",
    "\n",
    "You will want to use formula 5.43 in the course book to determine the updated weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PyTorch: 2-layer MLP for classification\n",
    "Luckily, there are some nice Deep Learning libraries out there, that make working with neural networks a pleasure.\n",
    "The two most noteable are [Tensorflow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org). We'll be using the latter.\n",
    "\n",
    "In order to install it in your conda environment you can use\n",
    "```pip install torch torchvision```\n",
    "\n",
    "A key feature of these libraries is that they can handle the gradient computation for you.\n",
    "Moreover, they have a lot of layer types and losses, that are easily composable to handle computation of complex neural networks.\n",
    "\n",
    "We'll be working with the classic MNIST dataset, which we can easily get via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few steps happened here:\n",
    "1. The dataset (train and test) was downloaded \n",
    "1. We created a `DataLoader` for each data split. Using this, we get batches of data (64 examples per batch here)\n",
    "1. We told asked for the training data to be shuffled\n",
    "\n",
    "Lets see what we get in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Create and train a 2-layer MLP\n",
    "For the network, use a single hidden layer of 512 neurons with a ReLU activation function for the first weight Linear layer. The output of the second Linear layer should be a softmax.\n",
    "\n",
    "For optimisation, use the SGD optimizer with learning rate of 0.001, and the negative log-likelihood loss.\n",
    "\n",
    "Train the network for 5 epochs on the train data, and report the prediction accuracy on the test data. You should be able to get about 90% correct.\n",
    "\n",
    "Hint: check the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for usage of the layers, optimizers and losses\n",
    "\n",
    "For this part of the exercise. You will want to replace each \"REPLACE\" with code correponding to the task described in the line above it. Such as \"#Initialise Linear Layers\", followed by \"REPLACE\", where you'd want to replace REPLACE with appropriate code for initialization of linear layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch.nn.functional import relu, log_softmax, nll_loss\n",
    "from torch.optim import SGD\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Initialise Linear layers\n",
    "        # <REPLACE>\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform feed-forward computation step\n",
    "        # <REPLACE>\n",
    "        output = ...\n",
    "        return output\n",
    "\n",
    "    \n",
    "def train(\n",
    "    model:Module, \n",
    "    train_loader:DataLoader, \n",
    "    optimizer: SGD, \n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reset the gradients\n",
    "        # <REPLACE>\n",
    "        \n",
    "        # Feed the data through the model\n",
    "        # <REPLACE>\n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        # <REPLACE>\n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        # <REPLACE>\n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        # <REPLACE>\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            # Log (Optional)\n",
    "            # <REPLACE>\n",
    "        \n",
    "\n",
    "\n",
    "def test(model:Module, test_loader:DataLoader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    # Don't accumulate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Feed the data through the model\n",
    "            # <REPLACE>\n",
    "            \n",
    "            # Predict the class (it is the index of the max log-probability)\n",
    "            # <REPLACE>\n",
    "            \n",
    "            # Add to the number of correct\n",
    "            correct += # <REPLACE>\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Print results\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, \n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch, log_interval=50)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
