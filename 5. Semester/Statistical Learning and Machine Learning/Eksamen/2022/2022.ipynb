{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Topics - Jan 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import exp, sqrt\n",
    "from collections import Counter\n",
    "from scipy.special import comb\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "from scipy.stats import binom\n",
    "from scipy.special import binom\n",
    "\n",
    "import scipy\n",
    "from sklearn.datasets import load_digits, make_swiss_roll\n",
    "from sklearn.decomposition import PCA \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) (Score: 20 %) List the parameters of the Gaussian mixture model that need to be estimated (from observed data) to fully specify the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Parameters to Estimate in the Gaussian Mixture Model (GMM) with K=2**:\n",
    "\n",
    "   For a GMM with $ K = 2 $ components modeling data $ x_i \\in \\mathbb{R}^2 $, the parameters to be estimated are:\n",
    "\n",
    "   - **Mixing Coefficients** ($ \\pi_k $): These are the probabilities associated with each Gaussian component in the mixture. Since there are two components, we need to estimate $ \\pi_1 $ and $ \\pi_2 $. These coefficients must sum up to 1.\n",
    "\n",
    "   - **Mean Vectors** ($ \\mu_k $): Each Gaussian component has a mean vector in $ \\mathbb{R}^2 $. We need to estimate $ \\mu_1 $ and $ \\mu_2 $.\n",
    "\n",
    "   - **Covariance Matrices** ($ \\Sigma_k $): Each component also has a $ 2 \\times 2 $ covariance matrix. We need to estimate $ \\Sigma_1 $ and $ \\Sigma_2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) (Score: 30 %) Assuming that the observed data points are i.i.d (independent and identically distributed) and that maximum likelihood estimation (MLE) is to be used for the estimation of model parameters, write the likelihood function that will be maximized within MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Likelihood Function for MLE**:\n",
    "\n",
    "   The likelihood function for a GMM is the product of the probability densities of each data point, given the model parameters. For N=400 and K=2, the likelihood function $ L $ is:\n",
    "\n",
    "   $$ L(\\pi, \\mu, \\Sigma) = \\prod_{i=1}^{400} \\left[ \\pi_1 \\mathcal{N}(x_i | \\mu_1, \\Sigma_1) + \\pi_2 \\mathcal{N}(x_i | \\mu_2, \\Sigma_2) \\right] $$\n",
    "\n",
    "   where $ \\mathcal{N}(x | \\mu_k, \\Sigma_k) $ is the probability density function of the Gaussian distribution with mean $ \\mu_k $ and covariance $ \\Sigma_k $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) (Score: 30 %) Briefly describe the methodology that you would adopt to estimate the mean vector µk of the k−th Gaussian component. Provide the mathematical expression for µk using the MLE\n",
    "\n",
    "3) **Methodology to Estimate Mean Vector $ \\mu_k $ Using MLE**:\n",
    "\n",
    "   - **Expectation-Maximization (EM) Algorithm**: Since direct maximization of the likelihood function in GMMs is not feasible, we use the EM algorithm.\n",
    "   \n",
    "   - **E-step**: Calculate the \"responsibilities\" $ \\gamma(z_{ik}) $, which represent the probability of data point $ x_i $ belonging to component $ k $ given the current parameter estimates.\n",
    "   \n",
    "   - **M-step**: Update $ \\mu_k $ using the responsibilities from the E-step. The update formula for $ \\mu_k $ is:\n",
    "\n",
    "     $$ \\mu_k = \\frac{\\sum_{i=1}^{400} \\gamma(z_{ik}) x_i}{\\sum_{i=1}^{400} \\gamma(z_{ik})} $$\n",
    "\n",
    "   - **Iterate**: Repeat E-step and M-step until convergence.\n",
    "\n",
    "   This is covered in p. 438-439 in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Score: 20 %) By using the MLE for the estimation of parameters of a Gaussian mixture model, do the resulting equations constitute a closed-form solution? If not, which scheme is used to solve the resulting set of equations and why?\n",
    "\n",
    "4) **Closed-Form Solution and Scheme for Solving MLE Equations**:\n",
    "\n",
    "   - **Closed-Form Solution**: No, the MLE for GMM does not yield a closed-form solution due to the mixture model's complexity and the dependence of the responsibilities on the parameters themselves.\n",
    "\n",
    "   - **EM Algorithm**: The EM algorithm is used to solve the resulting set of equations. It is an iterative approach that alternates between estimating the responsibilities (E-step) and updating the parameters (M-step) until convergence. The reason for using EM is that it simplifies the maximization problem by breaking it down into more manageable steps, each of which has a clear solution, and it deals effectively with the complexities arising from the mixture model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Validity of a Kernel Function**:\n",
    "\n",
    "   A function $ k(x, x') $ is a valid kernel function if it satisfies:\n",
    "   - **Positive Semi-Definiteness**: For any finite set of points $ \\{x_1, x_2, ..., x_n\\} $, the kernel matrix $ K $ formed by applying $ k $ to all pairs of these points must be positive semi-definite. This means for any non-zero vector $ a \\in \\mathbb{R}^n $, it must hold that $ a^T K a \\geq 0 $.\n",
    "\n",
    "2) **Validity of Polynomial Kernel $ k_1(x, x') = (x^Tx' + c)^2 $, $ c > 0 $ in Two-Dimensional Space**:\n",
    "\n",
    "   To verify the validity of the polynomial kernel for $ x = [x_1, x_2] $, we need to show it satisfies the above condition. For positive semi-definiteness, consider an arbitrary set of points $ \\{x^{(1)}, x^{(2)}, ..., x^{(n)}\\} $ and a vector $ a \\in \\mathbb{R}^n $. We need to show $ a^T K a \\geq 0 $:\n",
    "\n",
    "   - Expand $ k_1(x, x') $ for $ x, x' \\in \\mathbb{R}^2 $ and express it as a dot product of transformed vectors in a higher-dimensional space.\n",
    "   - The expanded form of $ (x^Tx' + c)^2 $ for two-dimensional $ x $ and $ x' $ will have terms like $ x_1^2x_1'^2, x_1x_2x_1'x_2', $ etc.\n",
    "   - By showing that this can be expressed as a dot product of some transformation of $ x $ and $ x' $, we ensure the kernel is a valid one due to Mercer's theorem, which states that a kernel is valid if it can be expressed as a dot product in some (possibly higher-dimensional) space.\n",
    "\n",
    "3) **Validity of Kernel $ k_2(x, x') = \\exp(k_1(x, x')) + k_1(x, x') $**:\n",
    "\n",
    "   To assess the validity of $ k_2 $:\n",
    "\n",
    "   - **Positive Semi-Definiteness**: Both terms in $ k_2 $ need to be positive semi-definite.\n",
    "   - The exponential of a kernel, $ \\exp(k_1(x, x')) $, is a valid kernel if $ k_1 $ is a valid kernel, as the exponential function applied element-wise to a positive semi-definite matrix results in a positive semi-definite matrix.\n",
    "   - The sum of two valid kernel functions is also a valid kernel function, as the sum of two positive semi-definite matrices is positive semi-definite.\n",
    "   - Therefore, if $ k_1 $ is valid (as established in part 2), then $ k_2(x, x') = \\exp(k_1(x, x')) + k_1(x, x') $ is also a valid kernel.\n",
    "\n",
    "In summary, the polynomial kernel $ k_1 $ is a valid kernel by Mercer's theorem, and $ k_2 $, being the sum of an exponential transformation of a valid kernel and the kernel itself, is also a valid kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **Importance of Initialization in K-Means Clustering**:\n",
    "\n",
    "   The initialization process in the K-Means method significantly impacts the final clustering result due to the following reasons:\n",
    "\n",
    "   - **Local Minima**: K-Means clustering aims to minimize the within-cluster sum of squares (WCSS). However, it can converge to different local minima based on the initial placement of centroids. Different initializations can lead to different local minima, resulting in varying clustering outcomes.\n",
    "\n",
    "   - **Convergence Speed**: Proper initialization can lead to faster convergence. If initial centroids are placed close to the final optimal positions, fewer iterations will be needed for the algorithm to converge.\n",
    "\n",
    "   - **Quality of Clusters**: The initial positioning of centroids can affect the compactness and separation of the resulting clusters. Poor initialization might lead to imbalanced clusters where some clusters may capture more data points while others capture very few.\n",
    "\n",
    "   - **Deterministic Results**: K-Means is a non-deterministic algorithm due to its random initialization. Methods like K-Means++ improve this by providing a more informed initialization process, leading to more consistent and potentially better clustering results.\n",
    "\n",
    "### 4) **Parameters in Mixture of Gaussians Affecting Clustering Compared to K-Means**:\n",
    "\n",
    "   When comparing the Mixture of Gaussians method to K-Means, the following parameters can lead to different clustering label assignments for data points:\n",
    "\n",
    "   - **Covariance Matrices (Σ)**: In a Gaussian Mixture Model (GMM), each cluster can have its own covariance matrix, which defines the shape and orientation of the cluster in the feature space. This flexibility allows GMM to fit more complex cluster shapes compared to K-Means, which assumes spherical clusters with equal variance in all directions.\n",
    "\n",
    "   - **Mixing Coefficients (π)**: GMMs incorporate mixing coefficients that define the prior probability of each Gaussian component (cluster). This affects the likelihood of data points belonging to different clusters, especially in overlapping regions, and can lead to different assignments compared to K-Means.\n",
    "\n",
    "   - **Means (μ)**: While both K-Means and GMMs estimate the means of clusters, the presence of different covariance matrices and mixing coefficients in GMMs can result in different mean estimations and, consequently, different cluster assignments.\n",
    "\n",
    "   - **Probabilistic Assignment**: GMM provides a probabilistic cluster assignment (soft clustering), which considers the probability of each data point belonging to each cluster. In contrast, K-Means assigns data points to the nearest cluster centroid (hard clustering), which can lead to different assignments, especially for points that are equidistant to multiple centroids or lie in regions where cluster boundaries are not well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the additional parameters and the probabilistic nature of GMM allow for more flexibility and complexity in capturing cluster characteristics, which can lead to different cluster assignments for certain data points compared to the simpler K-Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
